{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bed413-df6d-4e68-878f-7e5162c7e7f8",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444b9dea-b93b-4022-ab56-8938933fd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers ---- > Used to Minimize the Loss Functions\n",
    "# In ML we have 3 optimizers :\n",
    "\n",
    "# Gradient Descent --- > all data trained --- > prediction will close to actual point .\n",
    "# Stocastic G.D. ---- > 100 rows --- > 1 random row --- > y = mx + b\n",
    "# Mini Batch G.D. ---- > 100 rows --- > 4 batches (A=25,B=25,C=25,D=25) --- > random batch (B=25) --- > y = mx + b\n",
    "\n",
    "# In DL --- >\n",
    "# Stocastic G.D. (Issue local minima stuck) -- > solve momentum(0.9) , previous squared gradients claculation ---- > NAG(solve\n",
    "# issue of stucking in local minima and it calculates future squared gradients as well as past squared gradients) --- >\n",
    "# AdaGrad(Provide Automatic Learning rate) -- > solve RMSProp(Provide solution of automatic lr) --- >\n",
    "# Adam(combination of 2 optimizers a. Momentum b. RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ede7e-692e-4cf4-a923-f7f02cc6c61a",
   "metadata": {},
   "source": [
    "# Total Optimizers == >\n",
    "## In deep learning, optimizers are algorithms used to update the parameters of a neural network model during the training process. They play a crucial role in minimizing the loss function and improving the accuracy of the model. Here are several popular optimizers along with their\n",
    "\n",
    "## explanations:\n",
    "\n",
    "## (1). Stochastic Gradient Descent (SGD):\n",
    "### SGD is one of the simplest optimizers. It updates the parameters in the direction of the negative gradient of the loss function with respect to the current mini-batch of training data. However, SGD has a limitation of converging slowly and can get stuck in local minima.\n",
    "\n",
    "## (2). Momentum:\n",
    "### The Momentum optimizer builds upon SGD by adding a momentum term. It accumulates a fraction of the previous gradients to determine the direction of the update. This helps to accelerate convergence and navigate through flat regions and local minima.\n",
    "\n",
    "## (3). Nesterov Accelerated Gradient (NAG):\n",
    "### NAG is an improvement over the Momentum optimizer. It calculates the gradient not only based on the current parameters but also using an estimate of the future parameters. By looking ahead, NAG allows the optimizer to better anticipate the momentum's effect and adjust its update accordingly.\n",
    "\n",
    "## (4). AdaGrad:\n",
    "### AdaGrad adapts the learning rate for each parameter based on the historical gradients. It increases the learning rate for infrequent features and decreases it for frequent ones. This makes AdaGrad well-suited for sparse data but can cause the learning rate to become too small over time.\n",
    "\n",
    "## (5). RMSProp :\n",
    "### RMSProp addresses the diminishing learning rate issue of AdaGrad by introducing an exponentially decaying average of squared gradients . by keeping a moving average of the squared gradients, RMSprop normalizes the learning rate and improves convergence\n",
    "\n",
    "## (6). Adam(adaptive moment estimation)\n",
    "### Adam combines the concept of momentum and RMSProp . it maintain s an exponentiallyd decaying average of past gradients and squared gradients . Adam uses bias correction to account for theinitialization bias in the first few iterations , making it perform well in practise across different deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3ffbe-ab20-445d-8458-85ebb9a96934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
